{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "# Base directory\n",
    "base_dir = os.getcwd()\n",
    "# Initialize a list to hold the data\n",
    "data = []\n",
    "\n",
    "# Iterate over all subdirectories\n",
    "for directory in os.listdir(base_dir):\n",
    "    dir_path = os.path.join(base_dir, directory)\n",
    "    if os.path.isdir(dir_path):\n",
    "        try:\n",
    "            # Extract parameters from the directory name\n",
    "            params = directory.split('-')\n",
    "            output_dim = int(params[1])\n",
    "            hidden_dim = int(params[2])\n",
    "            num_layers = int(params[3])\n",
    "            dropout = float(params[4])\n",
    "\n",
    "            # Navigate into 'XJTU results'\n",
    "            xjtu_results_path = os.path.join(dir_path, 'XJTU results')\n",
    "            if os.path.isdir(xjtu_results_path):\n",
    "                # Iterate over batch subdirectories\n",
    "                for batch_dir in os.listdir(xjtu_results_path):\n",
    "                    batch_path = os.path.join(xjtu_results_path, batch_dir)\n",
    "                    if os.path.isdir(batch_path):\n",
    "                        # Extract batch number from the subdirectory name\n",
    "                        batch = batch_dir\n",
    "\n",
    "                        # Construct file paths\n",
    "                        red_label_path = os.path.join(batch_path, 'pred_label.npy')\n",
    "                        true_label_path = os.path.join(batch_path, 'true_label.npy')\n",
    "                        num_param_path = os.path.join(batch_path, 'num_param.txt')\n",
    "\n",
    "                        # Check if the required files exist\n",
    "                        if os.path.exists(red_label_path) and os.path.exists(true_label_path) and os.path.exists(num_param_path):\n",
    "                            # Read files\n",
    "                            red_label = np.load(red_label_path)\n",
    "                            true_label = np.load(true_label_path)\n",
    "                            with open(num_param_path, 'r') as f:\n",
    "                                num_param = int(f.read().strip())\n",
    "\n",
    "                            # Calculate metrics\n",
    "                            mae = mean_absolute_error(true_label, red_label)\n",
    "                            mape = mean_absolute_percentage_error(true_label, red_label)\n",
    "                            rmse = np.sqrt(mean_squared_error(true_label, red_label))\n",
    "\n",
    "                            # Append data\n",
    "                            data.append({\n",
    "                                'batch': batch,\n",
    "                                'output_dim': output_dim,\n",
    "                                'hidden_dim': hidden_dim,\n",
    "                                'num_layer': num_layers,\n",
    "                                'dropout': dropout,\n",
    "                                'num_param': num_param,\n",
    "                                'MAE': mae,\n",
    "                                'MAPE': mape,\n",
    "                                'RMSE': rmse\n",
    "                            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {directory}: {e}\")\n",
    "\n",
    "for i in range(6):\n",
    "\n",
    "    path1 = f\"../results of reviewer/XJTU results/{i}-{i}/\"\n",
    "\n",
    "    red_label = np.load(path1+\"/pred_label.npy\")\n",
    "    true_label = np.load(path1+\"/true_label.npy\")\n",
    "\n",
    "    mae = mean_absolute_error(true_label, red_label)\n",
    "    mape = mean_absolute_percentage_error(true_label, red_label)\n",
    "    rmse = np.sqrt(mean_squared_error(true_label, red_label))\n",
    "\n",
    "    data.append({\n",
    "        'batch': f\"{i}-{i}\",\n",
    "        'output_dim': -1,\n",
    "        'hidden_dim': -1,\n",
    "        'num_layer': -1,\n",
    "        'dropout': -1,\n",
    "        'num_param': 13662,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RMSE': rmse\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = df.sort_values(by=['batch', 'RMSE'], ascending=[True, True])\n",
    "\n",
    "# Display DataFrame\n",
    "df.to_csv('our-results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a model wins over the legacy model\n",
    "def check_win(row, base_row):\n",
    "    # Compare the metrics: MAE, MAPE, RMSE\n",
    "    metrics = ['MAE', 'MAPE', 'RMSE']\n",
    "    wins = sum(row[metric] < base_row[metric] for metric in metrics)\n",
    "    return wins >= 2\n",
    "\n",
    "# Group the data by batches\n",
    "batch_groups = df.groupby('batch')\n",
    "\n",
    "# List to collect results\n",
    "results = []\n",
    "\n",
    "# Iterate over each batch\n",
    "for batch, group in batch_groups:\n",
    "    # Get the base model row where dropout == -1\n",
    "    base_model_row = group[group['dropout'] == -1].squeeze()\n",
    "    \n",
    "    # Iterate over other rows within the same batch\n",
    "    for _, row in group.iterrows():\n",
    "        if row['dropout'] != -1:\n",
    "            # Check if the current model wins over the base model\n",
    "            if check_win(row, base_model_row):\n",
    "                # Extract hyperparameters and append to results\n",
    "                hyperparameters = row[['output_dim', 'hidden_dim', 'num_layer', 'dropout', 'num_param']]\n",
    "                results.append(tuple(hyperparameters))\n",
    "\n",
    "# Create a DataFrame from the results with a count of wins\n",
    "summary_df = pd.DataFrame(results, columns=['output_dim', 'hidden_dim', 'num_layer', 'dropout', 'num_param'])\n",
    "summary_df['win_over_legacy'] = 1\n",
    "summary_df = summary_df.groupby(['output_dim', 'hidden_dim', 'num_layer', 'dropout', 'num_param']).sum().reset_index()\n",
    "\n",
    "# Sort the new DataFrame by `win_over_legacy` in descending order\n",
    "sorted_summary_df = summary_df.sort_values(by='win_over_legacy', ascending=False)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "sorted_summary_df.to_csv(\"comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the specified hyperparameters and calculate the mean of the metrics\n",
    "grouped_df = df.groupby(['output_dim', 'hidden_dim', 'num_layer', 'dropout', 'num_param']).agg(\n",
    "    mean_MAE=('MAE', 'mean'),\n",
    "    mean_MAPE=('MAPE', 'mean'),\n",
    "    mean_RMSE=('RMSE', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Sort the new DataFrame by mean_RMSE in ascending order\n",
    "sorted_df = grouped_df.sort_values(by='mean_RMSE', ascending=True)\n",
    "\n",
    "# View the resulting DataFrame\n",
    "sorted_df.to_csv(\"comparison-2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
